先去
https://cmake.org/download/
下載安裝 Cmake

再在terminal
git clone https://github.com/ggerganov/llama.cpp

# 必須開一個新環境 不然原本的推論無法運行
pip install -r requirements.txt


可以輸入
python convert_hf_to_gguf.py -h
看參數說明

python convert_hf_to_gguf.py D:\PycharmProjects\LLM_Inference_test\models\Meta-Llama-3.2-1B-Instruct-hf-test --outfile D:\PycharmProjects\LLM_Inference_test\models\Meta-Llama-3.2-1B-Instruct-gguf-test --outtype f32

gguf 可以丟進 anythingLLM 或是 LM studio 等等應用程式進行推論

# 量化 Quantization
--outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}
use f32 for float32,
f16 for float16,
bf16 for bfloat16,
q8_0 for Q8_0,
tq1_0 or tq2_0 for ternary,
and auto for the highest-fidelity 16-bit float type depending on the first loaded tensor type




